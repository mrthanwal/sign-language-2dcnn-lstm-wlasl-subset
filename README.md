# Sign Language Recognition with 2D CNN + LSTM

A deep learning pipeline to recognize American Sign Language (ASL) signs from videos using a compact 2D CNN + LSTM model trained on a mini version of the WLASL dataset.

## ğŸš€ Project Overview
This project implements a lightweight and modular ASL recognition system that:
- Extracts and preprocesses videos from the WLASL dataset
- Selects meaningful frames
- Extracts hand landmarks using MediaPipe
- Trains a 2D CNN + LSTM model on sequences of landmarks
- Runs real-time inference on webcam input

Due to limited GPU resources, a mini-dataset was created and used instead of the full WLASL dataset.

## ğŸ“ Repository Structure
```
â”œâ”€â”€ class_list.txt              # List of selected ASL sign classes
â”œâ”€â”€ move_video.py               # Moves videos matching classes from WLASL metadata
â”œâ”€â”€ sort_videos.py              # Organizes videos into class folders
â”œâ”€â”€ frame_extract.py            # Extracts 15 frames per video with visible hands
â”œâ”€â”€ landmark_extract.py         # Extracts 3D hand landmarks using MediaPipe
â”œâ”€â”€ data_split.py               # Splits data into train, val, test sets
â”œâ”€â”€ final_model.py              # 2D CNN + LSTM training script
â”œâ”€â”€ model.h5                    # Trained model file
â”œâ”€â”€ test_model.py               # Real-time sign recognition via webcam
â””â”€â”€ README.md                   # Project documentation (this file)
```

## ğŸ“¦ Dataset
- **Original WLASL Dataset:** [WLASL Processed on Kaggle](https://www.kaggle.com/datasets/risangbaskoro/wlasl-processed)
- **Mini Dataset (Subset for this Project):** [Mini Dataset Link](http://www.something) *(to be updated)*

## âš™ï¸ Installation
```bash
# Clone this repo
$ git clone https://github.com/mrthanwal/sign-language-2dcnn-lstm-wlasl-subset.git
$ cd sign-language-2dcnn-lstm-wlasl-subset

# Create virtual environment (optional)
$ python -m venv venv
$ source venv/bin/activate  # On Windows: venv\Scripts\activate

```

> âš ï¸ Ensure you have MediaPipe, TensorFlow, OpenCV, and numpy installed.

## ğŸ§  Model Architecture
- **2D CNN**: Extracts features from each hand landmark frame (15 total)
- **LSTM**: Learns temporal sequences of sign motion
- **Input shape**: (15, 126, 1) where 126 is the MediaPipe hand landmark vector (2 hands Ã— 21 points Ã— 3 coords)

## ğŸ›  Usage

### Step 1: Prepare Mini Dataset
```bash
python move_video.py
python sort_videos.py
```

### Step 2: Preprocess Videos
```bash
python frame_extract.py
python landmark_extract.py
python data_split.py
```

### Step 3: Train the Model
```bash
python final_model.py
```

### Step 4: Test Model in Real-Time
```bash
python test_model.py
```

## âœ… Results
- Lightweight model suitable for low-resource environments
- Accurate real-time sign recognition with webcam input
- Better performance than 3D CNN baseline in limited compute conditions

## ğŸ”® Future Enhancements
- Convert to TensorFlow Lite for edge deployment
- Expand dataset with more classes and samples
- Use Transformer-based models for improved temporal learning
- Integrate a user-friendly GUI or TTS for accessibility

## ğŸ“„ License
This project is open-source under the [MIT License](LICENSE).

## ğŸ™Œ Acknowledgements
- WLASL Dataset authors
- MediaPipe by Google for hand landmark extraction

---

**Author:** [@mrthanwal](https://github.com/mrthanwal)
